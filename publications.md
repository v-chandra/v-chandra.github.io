---
layout: page
title: Selected Recent Publications
permalink: /publications/
---
1. Towards Zero-Shot Multilingual Transfer for Code-Switched Responses,
**ACL (2023)**. [[PDF]](https://aclanthology.org/2023.acl-long.417.pdf)

1. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2305.17888.pdf)

1. Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2306.04845.pdf)

1. XRBench: An Extended Reality (XR) Machine Learning Benchmark Suite for the Metaverse,
**MLSys (2023)**. [[PDF]](https://arxiv.org/pdf/2211.08675.pdf)

1. Fast Point Cloud Generation with Straight Flows,
**CVPR 2023**. [[PDF]](https://arxiv.org/pdf/2212.01747.pdf)

1. PathFusion: Path-consistent Lidar-Camera Deep Feature Fusion,
**arXiv (2022)**. [[PDF]](https://arxiv.org/pdf/2212.06244.pdf)

1. LiCo-Net: Linearized Convolution Network for Hardware-efficient Keyword Spotting,
**arXiv (2022)**. [[PDF]](https://arxiv.org/pdf/2211.04635.pdf)

1. Feature-align network with knowledge distillation for efficient denoising,
**WACV 2022**. [[PDF]](https://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Young_Feature-Align_Network_With_Knowledge_Distillation_for_Efficient_Denoising_WACVW_2022_paper.pdf)

1. NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training,
**ICLR 2022**. [[PDF]](https://openreview.net/pdf?id=Qaw16njk6L)

1. Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation,
**CVPR 2022**. [[PDF]](http://128.84.4.34/pdf/2111.01236)

1. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks,
**ICML 2022**. [[PDF]](https://arxiv.org/pdf/2206.00843.pdf)

1. Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet,
**ICASSP 2022**. [[PDF]](https://arxiv.org/pdf/2110.08352.pdf)

1. Streaming Parallel Transducer Beam Search with Fast-Slow Cascaded Encoders,
**INTERSPEECH 2022**. [[PDF]](https://www.isca-speech.org/archive/pdfs/interspeech_2022/mahadeokar22_interspeech.pdf)

1. ScaleNAS: Multi-Path One-Shot NAS for Scale-Aware High-Resolution Representation,
**AutoML 2022**. [[PDF]](https://openreview.net/pdf?id=BWfeZ6SIlq)

1. Contrastive Quant: Quantization makes Stronger Contrastive Learning,
**DAC 2022**. [[PDF]](https://dl.acm.org/doi/abs/10.1145/3489517.3530419)

1. Feature-Align Network with Knowledge Distillation for Efficient Denoising,
**WACV 2022**. [[PDF]](https://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Young_Feature-Align_Network_With_Knowledge_Distillation_for_Efficient_Denoising_WACVW_2022_paper.pdf)

1. CPT: Efficient Deep Neural Network Training via Cyclic Precision,
**ICLR 2021 (Spotlight Presentation)**. [[PDF]](https://arxiv.org/pdf/2101.09868.pdf)

1. AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling,
**CVPR 2021**. [[PDF]](https://arxiv.org/pdf/2011.09011.pdf)

1. KeepAugment: A Simple Information-Preserving Data Augmentation Approach,
**CVPR 2021**. [[PDF]](https://arxiv.org/pdf/2011.11778.pdf)

1. AlphaNet: Improved Training of Supernet with Alpha-Divergence,
**ICML 2021 (Long Presentation)**. [[PDF]](https://arxiv.org/pdf/2102.07954.pdf)

1. Double-win Quant: Aggressively Winning Robustness of Quantized Deep Neural Networks via Random Precision Training and Inference,
**ICML 2021**. [[PDF]](http://proceedings.mlr.press/v139/fu21c/fu21c.pdf)

1. NASGEM: Neural Architecture Search via Graph Embedding Method,
**AAAI 2021**. [[PDF]](https://arxiv.org/pdf/2007.04452.pdf)

1. Collaborative Training of Acoustic Encoders for Speech Recognition,
**INTERSPEECH 2021**. [[PDF]](https://arxiv.org/pdf/2106.08960.pdf)

1. Memory-efficient Speech Recognition on Smart Devices,
**ICASSP 2021**. [[PDF]](https://arxiv.org/pdf/2102.11531.pdf)

1. Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,
**HPCA 2021**. [[PDF]](https://arxiv.org/pdf/1909.07437.pdf)

1. EVRNet: Efficient Video Restoration on Edge Devices,
**International Conference on Multimedia, 2021**. [[PDF]](https://arxiv.org/pdf/2012.02228.pdf)

1. Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space Search,
**ASPLOS 2021**. [[PDF]](https://arxiv.org/pdf/2103.01489.pdf)

1. Noisy Training Improves E2E ASR for the Edge,
**arXiv (2021)**. [[PDF]](https://arxiv.org/pdf/2107.04677.pdf)

1. Low-Rank+ Sparse Tensor Compression for Neural Networks,
**arXiv (2021)**. [[PDF]](https://arxiv.org/pdf/2111.01697.pdf)

1. Vision Transformers with Patch Diversification,
**arXiv (2021)**. [[PDF]](https://arxiv.org/pdf/2104.12753.pdf)

1. Can Temporal Information Help with Contrastive Self-Supervised Learning?,
**arXiv (2020)**. [[PDF]](https://arxiv.org/pdf/2011.13046.pdf)

1. DNA: Differentiable Network-Accelerator Co-Search,
**arXiv (2020)**. [[PDF]](https://arxiv.org/pdf/2010.14778.pdf)

1. One weight bitwidth to rule them all,
**Embedded Vision Workshop, ECCV 2020 (Best Paper Award)**. [[PDF]](https://arxiv.org/pdf/2008.09916.pdf)

1. Co-Exploration of Neural Architectures and Heterogeneous ASIC Accelerator 
Designs Targeting Multiple Tasks,
**DAC 2020**. [[PDF]](https://arxiv.org/pdf/2002.04116.pdf)

1. RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing,
**ISCA 2020**. [[PDF]](https://arxiv.org/pdf/1912.12953.pdf)

1. Energy-Aware Neural Architecture Optimization With Splitting Steepest Descent, 
**Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS (2019)**. [[PDF]](https://arxiv.org/pdf/1910.03103.pdf)

1. Improving Efficiency in Neural Network Accelerator using Operands Hamming Distance Optimization,
**Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS (2019)**. [[PDF]](https://arxiv.org/pdf/2002.05293.pdf)

1. Federated Learning with Non-IID Data,
**arXiv (2018)**. [[PDF]](https://arxiv.org/pdf/1806.00582.pdf)

1. CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs,
**arXiv (2018)**. [[PDF]](https://arxiv.org/pdf/1801.06601.pdf)

1. Not All Ops are Created Equal!,
**SysML (2018)**. [[PDF]](https://arxiv.org/pdf/1801.04326.pdf)

1. PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,
**arXiv (2018)**. [[PDF]](https://arxiv.org/pdf/1709.06161.pdf)

1. Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks, 
**International Symposium on Computer Architecture, 2018**. [[PDF]](https://arxiv.org/pdf/1712.01507.pdf)

1. Hello Edge: Keyword Spotting on Microcontrollers, 
**arXiv (2017)**. [[PDF]](https://arxiv.org/pdf/1711.07128.pdf)

1. Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations,
**arXiv (2017)**. [[PDF]](https://arxiv.org/pdf/1703.03073.pdf)

1. Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks,
**FPGA Conference (2016)**. [[PDF]](https://dl.acm.org/citation.cfm?id=2847276)
