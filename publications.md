---
layout: page
title: Selected Recent Publications
permalink: /publications/
---
1. CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians,
**ECCV (2025)**. [[PDF]](https://arxiv.org/pdf/2403.19495)

1. Efficient Track Anything,
**arXiv (2024)**. [[PDF]](https://arxiv.org/pdf/2411.18933)

1. LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding,
**arXiv (2024)**. [[PDF]](https://arxiv.org/pdf/2410.17434)

1. Agent-as-a-Judge: Evaluate Agents with Agents,
**arXiv (2024)**. [[PDF]](https://arxiv.org/pdf/2410.10934)

1. High fidelity text-guided music generation and editing via single-stage flow matching,
**arXiv (2024)**. [[PDF]](https://arxiv.org/pdf/2407.03648)

1. MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,
**ICML 2024**. [[PDF]](https://arxiv.org/pdf/2402.14905)

1. EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything,
**CVPR 2024 (Highlight)**. [[PDF]](https://arxiv.org/pdf/2312.00863)

1. Taming Mode Collapse in Score Distillation for Text-to-3D Generation,
**CVPR 2024**. [[PDF]](https://arxiv.org/pdf/2401.00909.pdf)

1. CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians,
**ECCV (2024)**. [[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04306.pdf)

1. MVDiffHD: A Dense High-resolution Multi-view Diffusion Model for Single or 
Sparse-view 3D Object Reconstruction,
**ECCV (2024)**. [[PDF]](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02446.pdf)

1. TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-Device ASR Models,
**ICASSP 2024**. [[PDF]](https://arxiv.org/pdf/2309.01947)

1. Stack-and-delay: a new codebook pattern for music generation,
**ICASSP 2024**. [[PDF]](https://arxiv.org/pdf/2309.08804)

1. In-Context Prompt Editing for Conditional Audio Generation,
**ICASSP 2024**. [[PDF]](https://arxiv.org/pdf/2311.00895)

1. On the Open Prompt Challenge in Conditional Audio Generation,
**ICASSP 2024**. [[PDF]](https://arxiv.org/pdf/2311.00897)

1. Folding Attention: Memory and Power Optimization for On-Device Transformer-based Streaming
Speech Recognition,
**ICASSP 2024**. [[PDF]](https://arxiv.org/pdf/2309.07988)

1. LLM-QAT: Data-Free Quantization Aware Training for Large Language Models,
**ACL Findings (2024)**. [[PDF]](https://arxiv.org/pdf/2305.17888.pdf)

1. Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts,
**ACL Findings (2024)**. [[PDF]](https://arxiv.org/pdf/2306.04845.pdf)

1. An Introduction to Vision-Language Modeling,
**arXiv (2024)**. [[PDF]](https://arxiv.org/pdf/2405.17247)

1. PathFusion: Path-consistent Lidar-Camera Deep Feature Fusion,
**3D Vision (2024)**. [[PDF]](https://arxiv.org/pdf/2212.06244.pdf)

1. SpinQuant: LLM Quantization with Learned Rotations,
**arXiv (2024)**. [[PDF]](https://www.arxiv.org/pdf/2405.16406)

1. Basis Selection: Low-Rank Decomposition of Pretrained Large Language Models
for Target Applications,
**arXiv (2024)**. [[PDF]](https://arxiv.org/pdf/2405.15877)

1. DREAM: A Dynamic Scheduler for Dynamic Real-time Multi-model ML Workloads,
**ASPLOS 2024**. [[PDF]](https://arxiv.org/pdf/2212.03414)

1. SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity, 
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2401.00604.pdf)

1. MiniGPT-v2: Large Language Model As a Unified Interface for Vision-Language Multi-task Learning,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2310.09478.pdf)

1. Revisiting Sample Size Determination in Natural Language Understanding,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2307.00374.pdf)

1. Enhance audio generation controllability through representation similarity regularization,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2309.08773.pdf)

1. Exploring Speech Enhancement for Low-resource Speech Synthesis,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2309.10795.pdf)

1. FoleyGen: Visually-Guided Audio Generation,
**arXiv (2023)**. [[PDF]](https://arxiv.org/pdf/2309.10537.pdf)

1. Towards Zero-Shot Multilingual Transfer for Code-Switched Responses,
**ACL 2023**. [[PDF]](https://aclanthology.org/2023.acl-long.417.pdf)

1. XRBench: An Extended Reality (XR) Machine Learning Benchmark Suite for the Metaverse,
**MLSys 2023**. [[PDF]](https://arxiv.org/pdf/2211.08675.pdf)

1. Fast Point Cloud Generation with Straight Flows,
**CVPR 2023**. [[PDF]](https://arxiv.org/pdf/2212.01747.pdf)

1. LiCo-Net: Linearized Convolution Network for Hardware-efficient Keyword Spotting,
**arXiv (2022)**. [[PDF]](https://arxiv.org/pdf/2211.04635.pdf)

1. Feature-align network with knowledge distillation for efficient denoising,
**WACV 2022**. [[PDF]](https://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Young_Feature-Align_Network_With_Knowledge_Distillation_for_Efficient_Denoising_WACVW_2022_paper.pdf)

1. NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training,
**ICLR 2022**. [[PDF]](https://openreview.net/pdf?id=Qaw16njk6L)

1. Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation,
**CVPR 2022**. [[PDF]](http://128.84.4.34/pdf/2111.01236)

1. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks,
**ICML 2022**. [[PDF]](https://arxiv.org/pdf/2206.00843.pdf)

1. Omni-sparsity DNN: Fast Sparsity Optimization for On-Device Streaming E2E ASR via Supernet,
**ICASSP 2022**. [[PDF]](https://arxiv.org/pdf/2110.08352.pdf)

1. Streaming Parallel Transducer Beam Search with Fast-Slow Cascaded Encoders,
**INTERSPEECH 2022**. [[PDF]](https://www.isca-speech.org/archive/pdfs/interspeech_2022/mahadeokar22_interspeech.pdf)

1. ScaleNAS: Multi-Path One-Shot NAS for Scale-Aware High-Resolution Representation,
**AutoML 2022**. [[PDF]](https://openreview.net/pdf?id=BWfeZ6SIlq)

1. Contrastive Quant: Quantization makes Stronger Contrastive Learning,
**DAC 2022**. [[PDF]](https://dl.acm.org/doi/abs/10.1145/3489517.3530419)

1. Feature-Align Network with Knowledge Distillation for Efficient Denoising,
**WACV 2022**. [[PDF]](https://openaccess.thecvf.com/content/WACV2022W/WACI/papers/Young_Feature-Align_Network_With_Knowledge_Distillation_for_Efficient_Denoising_WACVW_2022_paper.pdf)

1. CPT: Efficient Deep Neural Network Training via Cyclic Precision,
**ICLR 2021 (Spotlight Presentation)**. [[PDF]](https://arxiv.org/pdf/2101.09868.pdf)

1. AttentiveNAS: Improving Neural Architecture Search via Attentive Sampling,
**CVPR 2021**. [[PDF]](https://arxiv.org/pdf/2011.09011.pdf)

1. KeepAugment: A Simple Information-Preserving Data Augmentation Approach,
**CVPR 2021**. [[PDF]](https://arxiv.org/pdf/2011.11778.pdf)

1. AlphaNet: Improved Training of Supernet with Alpha-Divergence,
**ICML 2021 (Long Presentation)**. [[PDF]](https://arxiv.org/pdf/2102.07954.pdf)

1. Double-win Quant: Aggressively Winning Robustness of Quantized Deep Neural Networks via Random Precision Training and Inference,
**ICML 2021**. [[PDF]](http://proceedings.mlr.press/v139/fu21c/fu21c.pdf)

1. NASGEM: Neural Architecture Search via Graph Embedding Method,
**AAAI 2021**. [[PDF]](https://arxiv.org/pdf/2007.04452.pdf)

1. Collaborative Training of Acoustic Encoders for Speech Recognition,
**INTERSPEECH 2021**. [[PDF]](https://arxiv.org/pdf/2106.08960.pdf)

1. Memory-efficient Speech Recognition on Smart Devices,
**ICASSP 2021**. [[PDF]](https://arxiv.org/pdf/2102.11531.pdf)

1. Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,
**HPCA 2021**. [[PDF]](https://arxiv.org/pdf/1909.07437.pdf)

1. EVRNet: Efficient Video Restoration on Edge Devices,
**International Conference on Multimedia, 2021**. [[PDF]](https://arxiv.org/pdf/2012.02228.pdf)

1. Mind Mappings: Enabling Efficient Algorithm-Accelerator Mapping Space Search,
**ASPLOS 2021**. [[PDF]](https://arxiv.org/pdf/2103.01489.pdf)

1. Noisy Training Improves E2E ASR for the Edge,
**arXiv (2021)**. [[PDF]](https://arxiv.org/pdf/2107.04677.pdf)

1. Low-Rank+ Sparse Tensor Compression for Neural Networks,
**arXiv (2021)**. [[PDF]](https://arxiv.org/pdf/2111.01697.pdf)

1. Vision Transformers with Patch Diversification,
**arXiv (2021)**. [[PDF]](https://arxiv.org/pdf/2104.12753.pdf)

1. Can Temporal Information Help with Contrastive Self-Supervised Learning?,
**arXiv (2020)**. [[PDF]](https://arxiv.org/pdf/2011.13046.pdf)

1. DNA: Differentiable Network-Accelerator Co-Search,
**arXiv (2020)**. [[PDF]](https://arxiv.org/pdf/2010.14778.pdf)

1. One weight bitwidth to rule them all,
**Embedded Vision Workshop, ECCV 2020 (Best Paper Award)**. [[PDF]](https://arxiv.org/pdf/2008.09916.pdf)

1. Co-Exploration of Neural Architectures and Heterogeneous ASIC Accelerator 
Designs Targeting Multiple Tasks,
**DAC 2020**. [[PDF]](https://arxiv.org/pdf/2002.04116.pdf)

1. RecNMP: Accelerating Personalized Recommendation with Near-Memory Processing,
**ISCA 2020**. [[PDF]](https://arxiv.org/pdf/1912.12953.pdf)

1. Energy-Aware Neural Architecture Optimization With Splitting Steepest Descent, 
**Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS (2019)**. [[PDF]](https://arxiv.org/pdf/1910.03103.pdf)

1. Improving Efficiency in Neural Network Accelerator using Operands Hamming Distance Optimization,
**Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS (2019)**. [[PDF]](https://arxiv.org/pdf/2002.05293.pdf)

1. Federated Learning with Non-IID Data,
**arXiv (2018)**. [[PDF]](https://arxiv.org/pdf/1806.00582.pdf)

1. CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs,
**arXiv (2018)**. [[PDF]](https://arxiv.org/pdf/1801.06601.pdf)

1. Not All Ops are Created Equal!,
**SysML (2018)**. [[PDF]](https://arxiv.org/pdf/1801.04326.pdf)

1. PrivyNet: A Flexible Framework for Privacy-Preserving Deep Neural Network Training,
**arXiv (2018)**. [[PDF]](https://arxiv.org/pdf/1709.06161.pdf)

1. Bit Fusion: Bit-Level Dynamically Composable Architecture for Accelerating Deep Neural Networks, 
**International Symposium on Computer Architecture, 2018**. [[PDF]](https://arxiv.org/pdf/1712.01507.pdf)

1. Hello Edge: Keyword Spotting on Microcontrollers, 
**arXiv (2017)**. [[PDF]](https://arxiv.org/pdf/1711.07128.pdf)

1. Deep Convolutional Neural Network Inference with Floating-point Weights and Fixed-point Activations,
**arXiv (2017)**. [[PDF]](https://arxiv.org/pdf/1703.03073.pdf)

1. Throughput-optimized OpenCL-based FPGA accelerator for large-scale convolutional neural networks,
**FPGA Conference (2016)**. [[PDF]](https://dl.acm.org/citation.cfm?id=2847276)
